
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from concurrent.futures import ThreadPoolExecutor
from langchain_community.vectorstores import FAISS
import warnings
import subprocess
import json


warnings.filterwarnings("ignore", category=FutureWarning, message="`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.")



PROMPT_TEMPLATE = []

PROMPT_TEMPLATE.append("""
You are a knowledgeable assistant and a SQL database and relations expert.
 
Your task is to answer the following question  about C2M by interpreting the context given below.
 
Context: 
{context}
                       
Conversation History: 
{history}                       
 
Question: {question}
 
Answer in detail:
""")

PROMPT_TEMPLATE.append("""    
You are a knowledgeable assistant and a SQL database and relations expert.
 
Your task is to answer the following question by interpreting metadata and configuration instructions to accurately read, understand or update the database. 
 
Follow these guidelines:
 
1. Identify the tables asked to configure and generate values for all field names in the table using INSERT format.
2. Also update parent and child tables keeping primary and foreign keys in mind, Eg: If you are configuring a child table its primary key comes from the parent table, the parent table row should also be inserted. Similarly all child tables should also update accordingly.
3. Provide complete and detailed answers without using ellipses ("..."). 
4. Avoid unnecessary conversation and stick to the point.
 
Stick to this Schema Information:
{table_info}
                        
Conversation History: 
{history}
 
Question: {question}
 
Answer in detail:
""")

PROMPT_TEMPLATE.append("""
                       
Conversation History: 
{history}
                       
You are a knowledgeable assistant and a SQL database and relations expert. Your task is to interpret metadata and configuration instructions to accurately read, understand or update the database tables by generating field values. Follow these guidelines:
 
1. Identify the tables asked to configure and generate values for all field names in the table using INSERT format.
2. Also update parent and child tables keeping primary and foreign keys in mind, Eg: If you are configuring a child table its primary key comes from the parent table, the parent table row should also be inserted. Similarly all child tables should also update accordingly.
3. Provide complete and detailed answers without using ellipses ("..."). (If you're asked for 100 rows you will give 100 rows of data. Don't be lazy.)
4. Avoid unnecessary conversation and stick to the point.
 
Stick to this Schema Information:
{table_info}              
 
Use precise table and field names as provided in the schema. Match whole name for table names and field names. For example, if the table name is "users" it is not the same as "user".
 
##RULES##
    -Use the MM-DD-YYYY format for dates.
    -All rows to be added must be in insert query format and all rows values must be generated by you. Do not provide logic code for it. Provide direct INSERT query with multiple rows. 
    -English language is used 'ENG'.
                    
                        
Question: {question}

Answer in detail:
"""
)

kArray=[5,3,3]
kArrayTbl=[1,3,1]
model = Ollama(model="llama3:instruct", temperature=0.2)
embedding_function = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2',model_kwargs={ "device":"cuda" }) #HuggingFaceEmbeddings("sentence-transformers/msmarco-distilroberta-base-v2")
db = FAISS.load_local("faiss_index", embedding_function, allow_dangerous_deserialization=True)
db_tbl = FAISS.load_local("faiss_index_tbl", embedding_function, allow_dangerous_deserialization=True)


def build_context(results):
    context_texts = []

    def process_result(doc_score):
        doc, _score = doc_score
        return doc.page_content

    with ThreadPoolExecutor() as executor:
        context_texts = list(executor.map(process_result, results))

    context_text = "\n\n---\n\n".join(context_texts)
    return context_text

def build_table_info(results):
    context_texts = []

    def process_result(doc_score):
        doc, _score = doc_score
        return doc.page_content

    with ThreadPoolExecutor() as executor:
        context_texts = list(executor.map(process_result, results))

    context_text = "\n\n---\n\n".join(context_texts)
    return context_text


def query_rag(query_text,history,promptIndex):
    
    # Search the DB in parallel
    with ThreadPoolExecutor() as executor:
        future_results = executor.submit(db.similarity_search_with_score, query_text,k=kArray[promptIndex])
        results = future_results.result()
        future_results_tbl = executor.submit(db_tbl.similarity_search_with_score, query_text, k=kArrayTbl[promptIndex])
        results_tbl = future_results_tbl.result()
        

    # Build context in parallel
    context_text = build_context(results)
    # print("instruction context: ", context_text)

    context_text_tbl = build_context(results_tbl)

    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE[promptIndex])
    prompt = prompt_template.format(context=context_text, table_info=context_text_tbl, question=query_text, history=history)

    response_text = model.invoke(prompt)

    sources = [doc.metadata.get("id", None) for doc, _score in results]
    sources.append([doc.metadata.get("id", None) for doc, _score in results_tbl])
    
    return [response_text, sources]

def get_user_feedback():
    while True:
        feedback = input("Rate the response (1-5): ")
        if feedback in ['1', '2', '3', '4', '5']:
            return int(feedback)
        else:
            print("Invalid input, please enter a number between 1 and 5.")

if __name__ == "__main__":
    inp=""
    history=[]
    response=""
    ind=0
    pfs=["C2M info","DB info","DB config help"]
    print("Enter 'quit' to exit")
    print("Enter: \n pf0 for C2M info \n pf1 for DB info \n pf2 for DB config help \n\nDefault prompt template is C2M info")
    while(True):
        
        inp=input("\nQuery: ")
        if(inp=="quit"):
            break
        if(inp=="pf0"or inp=="pf1" or inp=="pf2"):
            ind=int(inp[-1])
            print("Prompt Format: ",pfs[ind])
            continue
        response=query_rag(inp,str(history),ind)
        print("Response: ",response[0])
        print("Sources: ",response[1])
        feedback = get_user_feedback()

        history.append({"role": "user", "content": inp, "feedback": feedback})
        history.append({"role": "assistant", "content": response[0], "sources": response[1], "feedback": feedback})

        with open("history.json", "w") as f:
            json.dump(history, f)

        # Fine-tune the model using PPO based on the feedback
        subprocess.run(["python", "ppo_finetune.py"])

    print("Exiting...")

