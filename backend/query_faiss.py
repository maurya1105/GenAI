import argparse
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from concurrent.futures import ThreadPoolExecutor
from langchain_community.vectorstores import FAISS
import warnings
from get_embedding_function2 import get_embedding_function
from datetime import datetime


warnings.filterwarnings("ignore", category=FutureWarning, message="`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.")

PROMPT_TEMPLATE = """
You are a knowledgeable assistant and a SQL database and relations expert. Your task is to interpret metadata and configuration instructions to accurately read, understand or update the database tables by generating field values. Follow these guidelines:
 
1. Identify the tables asked to configure and generate values for all field names in the table using INSERT format.
2. Also update parent and child tables keeping primary and foreign keys in mind, Eg: If you are configuring a child table its primary key comes from the parent table, the parent table row should also be inserted. Similarly all child tables should also update accordingly.
3. Provide complete and detailed answers without using ellipses ("..."). (If you're asked for 100 rows you will give 100 rows of data. Don't be lazy.)
4. Avoid unnecessary conversation and stick to the point.
 
Stick to this Schema Information:
{table_info}

Use precise table and field names as provided in the schema. Match whole name for table names and field names. For example, if the table name is "users" it is not the same as "user".
 
##RULES##
    -Use the YYYY-MM-DD format for dates.
    -All rows to be added must be in insert query format and all rows values must be generated by you. Do not provide logic code for it. Provide direct INSERT query with multiple rows. 
    -English language is used 'ENG'.
    -Assume tables to already be created, only provide insert queries.

Question: {question}
 
Answer in detail:
"""

model = Ollama(model="llama3:instruct", temperature=0)
embedding_function = get_embedding_function()
db = FAISS.load_local("faiss_index", embedding_function, allow_dangerous_deserialization=True)
db_tbl = FAISS.load_local("faiss_index_tbl", embedding_function, allow_dangerous_deserialization=True)

def main():
    
    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    args = parser.parse_args()
    query_text = args.query_text
    query_rag(query_text)
    

def build_context(results):
    context_texts = []

    def process_result(doc_score):
        doc, _score = doc_score
        return doc.page_content

    with ThreadPoolExecutor() as executor:
        context_texts = list(executor.map(process_result, results))

    context_text = "\n\n---\n\n".join(context_texts)
    return context_text

def build_table_info(results):
    context_texts = []

    def process_result(doc_score):
        doc, _score = doc_score
        return doc.page_content

    with ThreadPoolExecutor() as executor:
        context_texts = list(executor.map(process_result, results))

    context_text = "\n\n---\n\n".join(context_texts)
    return context_text


def query_rag(query_text: str):

    # Search the DB in parallel
    with ThreadPoolExecutor() as executor:
        future_results = executor.submit(db.similarity_search_with_score, query_text, k=3)
        results = future_results.result()
        future_results_tbl = executor.submit(db_tbl.similarity_search_with_score, query_text, k=1)
        results_tbl = future_results_tbl.result()
        

    # Build context in parallel
    context_text = build_context(results)
    # print("instruction context: ", context_text)

    context_text_tbl = build_context(results_tbl)
    print("Tbl info context: ", context_text_tbl)

    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, table_info=context_text_tbl, question=query_text)

    response_text = model.invoke(prompt)

    sources = [doc.metadata.get("id", None) for doc, _score in results]
    sources.append([doc.metadata.get("id", None) for doc, _score in results_tbl])
    formatted_response = f"Response: {response_text}\n\nSources: {sources}"
    print(formatted_response)
    return response_text



if __name__ == "__main__":
    start_time = datetime.now()
    main()
    end_time = datetime.now()
    execution_time = end_time - start_time
    print(f"Execution time: {execution_time}")
